{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import numpy.ma as ma\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = open(\"dataset/atis-2.train.w-intent.iob\", \"r\").readlines()\n",
    "test_data = open(\"dataset/atis-2.dev.w-intent.iob\", \"r\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BOS i want to fly from baltimore to dallas round trip EOS\\tO O O O O O B-fromloc.city_name O B-toloc.city_name B-round_trip I-round_trip atis_flight\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(datas, max_len = 50):\n",
    "    \n",
    "    # split to word_sequence, slot_sequence, intent\n",
    "    datas = [data.strip() for data in datas]\n",
    "    datas = [[data.split('\\t')[0].split()[1:-1], data.split('\\t')[1].split()[:-1], data.split()[-1]] for data in datas]\n",
    "    seq_words, seq_slots, intents = list(zip(*datas))\n",
    "    \n",
    "    # add special tokens\n",
    "    seq_new_words, seq_new_slots = [],[]\n",
    "    for num in range(len(seq_words)):\n",
    "        temp = seq_words[num]\n",
    "        \n",
    "        if len(temp)<max_len:\n",
    "            temp.append('<EOS>')\n",
    "            while len(temp)<max_len:\n",
    "                temp.append('<PAD>')\n",
    "        else:\n",
    "            temp = temp[:max_len]\n",
    "            temp[-1] = '<EOS>'\n",
    "        seq_new_words.append(temp)\n",
    "        \n",
    "        temp = seq_slots[num]\n",
    "        if len(temp)<max_len:\n",
    "            while len(temp)<max_len:\n",
    "                temp.append('<PAD>')\n",
    "        else:\n",
    "            temp = temp[:max_len]\n",
    "            \n",
    "        seq_new_slots.append(temp)\n",
    "        \n",
    "    datas = list(zip(seq_new_words, seq_new_slots, intents))\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping_dictionary(datas):\n",
    "    \n",
    "    seq_words, seq_slots, intents = list(zip(*datas))\n",
    "    \n",
    "    # flatten function\n",
    "    flatten = lambda l:[item for seq in l for item in seq]\n",
    "    \n",
    "    # build word2id and id2word\n",
    "    word2id = {\"<PAD>\":0, '<UNK>':1, '<SOS>':2, '<EOS>':3}\n",
    "    for word in set(flatten(seq_words)):\n",
    "        if word not in word2id:\n",
    "            word2id[word] = len(word2id)\n",
    "    id2word = {v:k for k,v in word2id.items()}\n",
    "    \n",
    "    # build slot2id and id2slot\n",
    "    slot2id = {'<PAD>':0, '<UNK>':1, 'O':2}\n",
    "    for slot in set(flatten(seq_slots)):\n",
    "        if slot not in slot2id:\n",
    "            slot2id[slot] = len(slot2id)\n",
    "    id2slot = {v:k for k,v in slot2id.items()}\n",
    "    \n",
    "    # build intent2id and id2intent\n",
    "    intent2id = {'<UNK>':0}\n",
    "    for intent in set(intents):\n",
    "        if intent not in intent2id:\n",
    "            intent2id[intent] = len(intent2id)\n",
    "    id2intent = {v:k for k,v in intent2id.items()}\n",
    "    \n",
    "    return word2id, id2word, slot2id, id2slot, intent2id, id2intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_index(datas, word2id, slot2id, intent2id):\n",
    "    \n",
    "    data_with_id = []\n",
    "    for seq_words, seq_slots, intent in datas:\n",
    "        seq_words_id = list(map(lambda i:word2id.get(i, word2id['<UNK>']), seq_words))\n",
    "        seq_len = seq_words.index('<EOS>')\n",
    "        seq_slots_id = list(map(lambda i:slot2id.get(i, slot2id['<UNK>']), seq_slots))\n",
    "        intents_id = intent2id.get(intent, intent2id['<UNK>'])\n",
    "        data_with_id.append([seq_words_id, seq_len, seq_slots_id, intents_id])\n",
    "    \n",
    "    return data_with_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_ed = data_pipeline(train_data)\n",
    "test_data_ed = data_pipeline(test_data)\n",
    "\n",
    "word2id, id2word, slot2id, id2slot, intent2id, id2intent = get_mapping_dictionary(train_data_ed)\n",
    "train_data_with_id = to_index(train_data_ed, word2id, slot2id, intent2id)\n",
    "test_data_with_id = to_index(test_data_ed, word2id, slot2id, intent2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['i', 'want', 'to', 'fly', 'from', 'baltimore', 'to', 'dallas', 'round', 'trip', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'B-round_trip', 'I-round_trip', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], 'atis_flight')\n",
      "\n",
      "[[89, 659, 859, 190, 206, 164, 859, 616, 840, 812, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 10, [2, 2, 2, 2, 2, 2, 101, 2, 117, 25, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 5]\n"
     ]
    }
   ],
   "source": [
    "print(train_data_ed[0])\n",
    "print()\n",
    "print(train_data_with_id[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameter/input/output/variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 5\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "input_steps = 50\n",
    "\n",
    "vocab_size = 871\n",
    "\n",
    "embed_size = 64\n",
    "\n",
    "hidden_size = 100\n",
    "\n",
    "slot_size = 122\n",
    "\n",
    "intent_size = 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input/Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = tf.placeholder(tf.int32, [input_steps, batch_size], name = 'encoder_input')\n",
    "encoder_len = tf.placeholder(tf.int32, [batch_size], name = 'encoder_len')\n",
    "slot_target = tf.placeholder(tf.int32, [batch_size, input_steps], name = 'slot_target')\n",
    "intent_target = tf.placeholder(tf.int32, [batch_size], name = 'intent_target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, embed_size], -1.0, 1.0), dtype = tf.float32, name = 'embeddings')\n",
    "encoder_embed_input = tf.nn.embedding_lookup(embeddings, encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot_w = tf.Variable(tf.random_uniform([hidden_size*2, slot_size], -1.0, 1.0), dtype = tf.float32, name = 'slot_w')\n",
    "slot_b = tf.zeros([slot_size], dtype = tf.float32, name = 'slot_b')\n",
    "intent_w = tf.Variable(tf.random_uniform([hidden_size*2, intent_size], -1.0, 1.0), dtype = tf.float32, name = 'intent_w')\n",
    "intent_b = tf.zeros([intent_size], dtype = tf.float32, name = 'intent_b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### special token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_time_slice = tf.ones([batch_size], dtype = tf.int32)*2\n",
    "sos_embed = tf.nn.embedding_lookup(embeddings, sos_time_slice, name = 'SOS')\n",
    "pad_embed = tf.zeros([batch_size, hidden_size*2+embed_size], dtype = tf.float32, name = 'PAD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_embed_input.shape:(50, 16, 64)\n",
      "sos_embed.shape:(16, 64)\n"
     ]
    }
   ],
   "source": [
    "print('encoder_embed_input.shape:{}'.format(encoder_embed_input.shape))\n",
    "print('sos_embed.shape:{}'.format(sos_embed.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_cell = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "((encoder_fw_output, encoder_bw_output), \n",
    " (encoder_fw_final_state, encoder_bw_final_state)) = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell,\n",
    "                                                                                     cell_bw = encoder_cell,\n",
    "                                                                                     inputs = encoder_embed_input,\n",
    "                                                                                     sequence_length = encoder_len,\n",
    "                                                                                     dtype = tf.float32,\n",
    "                                                                                     time_major = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = tf.concat((encoder_fw_output, encoder_bw_output), 2)\n",
    "encoder_final_state_c = tf.concat((encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "encoder_final_state_h = tf.concat((encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "encoder_final_state = tf.contrib.rnn.LSTMStateTuple(c = encoder_final_state_c,\n",
    "                                                    h = encoder_final_state_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_output.shape:(50, 16, 200)\n",
      "encoder_final_state_c.shape:(16, 200)\n",
      "encoder_final_state_h.shape:(16, 200)\n",
      "encoder_final_state:LSTMStateTuple(c=<tf.Tensor 'concat_1:0' shape=(16, 200) dtype=float32>, h=<tf.Tensor 'concat_2:0' shape=(16, 200) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "print('encoder_output.shape:{}'.format(encoder_output.shape))\n",
    "print('encoder_final_state_c.shape:{}'.format(encoder_final_state_c.shape))\n",
    "print('encoder_final_state_h.shape:{}'.format(encoder_final_state_h.shape))\n",
    "print('encoder_final_state:{}'.format(encoder_final_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_len = encoder_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build custom helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_fn():\n",
    "    \n",
    "    finished = (0>=decoder_len)\n",
    "    initial_input = tf.concat((sos_embed, encoder_output[0]), 1)\n",
    "    \n",
    "    return finished, initial_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_id(time, outputs, state):\n",
    "    \n",
    "    pred_id = tf.to_int32(tf.argmax(outputs, 1))\n",
    "    \n",
    "    return pred_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_translate_fn(time, outputs, state, sample_ids):\n",
    "    \n",
    "    def get_next_input():\n",
    "        \n",
    "        pred_embed = tf.nn.embedding_lookup(embeddings, sample_ids)\n",
    "        \n",
    "        return tf.concat((pred_embed, encoder_output[time]), 1)\n",
    "    \n",
    "    finished = (time>=decoder_len)\n",
    "    all_finished = tf.reduce_all(finished)\n",
    "    next_input = tf.cond(all_finished, lambda:pad_embed, get_next_input)\n",
    "    next_state = state\n",
    "    \n",
    "    return finished, next_input, next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_helper = tf.contrib.seq2seq.CustomHelper(initial_fn, get_sample_id, loop_translate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build decode flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(helper, scope, reuse = None):\n",
    "    with tf.variable_scope(scope, reuse = reuse):\n",
    "        \n",
    "        # build attention mechanism\n",
    "        memory = tf.transpose(encoder_output, [1, 0 ,2])\n",
    "        attn_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units = hidden_size,\n",
    "                                                              memory = memory,\n",
    "                                                              memory_sequence_length = encoder_len)\n",
    "        \n",
    "        # build attention cell\n",
    "        cell = tf.contrib.rnn.LSTMCell(hidden_size*2)\n",
    "        attn_cell = tf.contrib.seq2seq.AttentionWrapper(cell = cell,\n",
    "                                                        attention_mechanism = attn_mechanism,\n",
    "                                                        attention_layer_size = hidden_size)\n",
    "        \n",
    "        # build output projection\n",
    "        out_cell = tf.contrib.rnn.OutputProjectionWrapper(cell = attn_cell,\n",
    "                                                          output_size = slot_size,\n",
    "                                                          reuse = reuse)\n",
    "        \n",
    "        # build decoder\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(cell = out_cell,\n",
    "                                                  initial_state = out_cell.zero_state(batch_size = batch_size, dtype = tf.float32),\n",
    "                                                  helper = helper)\n",
    "        \n",
    "        # run decoder\n",
    "        (final_output, final_state, seq_len) = tf.contrib.seq2seq.dynamic_decode(decoder = decoder,\n",
    "                                                                                 output_time_major = True,\n",
    "                                                                                 impute_finished = True,\n",
    "                                                                                 maximum_iterations = input_steps)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_decoder_output = decode(my_helper, 'decode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred slot id shape:(?, 16)\n",
      "pred slot logit shape:(?, 16, 122)\n"
     ]
    }
   ],
   "source": [
    "print('pred slot id shape:{}'.format(final_decoder_output.sample_id.shape))\n",
    "print('pred slot logit shape:{}'.format(final_decoder_output.rnn_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process slot prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot_prediction_id = final_decoder_output.sample_id\n",
    "slot_prediction_logits = final_decoder_output.rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "(decoder_max_steps, decoder_batch_size, decoder_dim) = tf.unstack(tf.shape(slot_prediction_logits))\n",
    "slot_target_with_time_major = tf.transpose(slot_target, [1, 0])\n",
    "slot_target_with_time_major_and_true_len = slot_target_with_time_major[:decoder_max_steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tf.to_float(tf.not_equal(slot_target_with_time_major_and_true_len, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot_loss = tf.contrib.seq2seq.sequence_loss(logits = slot_prediction_logits,\n",
    "                                             targets = slot_target_with_time_major_and_true_len,\n",
    "                                             weights = mask,\n",
    "                                             name = 'slot_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slot_target_with_time_major_and_true_len.shape:(?, 16)\n",
      "mask:Tensor(\"ToFloat:0\", shape=(?, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('slot_target_with_time_major_and_true_len.shape:{}'.format(slot_target_with_time_major_and_true_len.shape))\n",
    "print('mask:{}'.format(mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process intent prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_prediction_logits = tf.matmul(encoder_final_state_h, intent_w)+intent_b\n",
    "intent_prediction_id = tf.argmax(intent_prediction_logits, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = intent_prediction_logits,\n",
    "                                                         labels = tf.one_hot(indices = intent_target,\n",
    "                                                                             depth = intent_size,\n",
    "                                                                             dtype = tf.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = slot_loss+intent_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "grads, vars = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(grads, 5)\n",
    "train_op = optimizer.apply_gradients(zip(grads, vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(datas, batch_size):\n",
    "    \n",
    "    random.shuffle(datas)\n",
    "    start_index = 0\n",
    "    end_index = batch_size\n",
    "    \n",
    "    while end_index<len(datas):\n",
    "        batch = datas[start_index:end_index]\n",
    "        start_index, end_index = end_index, end_index+batch_size\n",
    "        \n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_index2word = lambda seq, id2word:' '.join([id2word[word] for word in seq])\n",
    "seq_index2slot = lambda seq, id2slot:' '.join([id2slot[slot] for slot in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(true_data, pred_data, true_len = None):\n",
    "    \n",
    "    true_data = np.array(true_data)\n",
    "    pred_data = np.array(pred_data)\n",
    "\n",
    "    assert true_data.shape == pred_data.shape\n",
    "    \n",
    "    if true_len is not None:\n",
    "        val_num = np.sum(true_len)\n",
    "        assert val_num!=0\n",
    "        score = 0\n",
    "        \n",
    "        for i in range(true_data.shape[0]):\n",
    "            score+=np.sum(true_data[i, :true_len[i]] == pred_data[i, :true_len[i]])\n",
    "    else:\n",
    "        val_num = np.prod(true_data.shape)\n",
    "        score = np.sum(true_data == pred_data)\n",
    "    \n",
    "    accuracy_score = score/float(val_num)\n",
    "    \n",
    "    return accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_seq_batch(true_batch, pred_batch, padding_token):\n",
    "\n",
    "    true_ma = ma.masked_equal(true_batch, padding_token)\n",
    "    pred_ma = ma.masked_array(pred_batch, true_ma.mask)\n",
    "    true_ma = true_ma.flatten()\n",
    "    pred_ma = pred_ma.flatten()\n",
    "    true_ma = true_ma[~true_ma.mask]\n",
    "    pred_ma = pred_ma[~pred_ma.mask]\n",
    "    \n",
    "    return true_ma, pred_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_for_seq_batch(true_batch, pred_batch, average = 'micro', padding_token = 0):\n",
    "    \n",
    "    true_data, pred_data = get_data_from_seq_batch(true_batch, pred_batch, padding_token)\n",
    "    labels = list(set(true_data))\n",
    "    return f1_score(true_data, pred_data, labels = labels, average = average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, train_loss:1.6559512047998366\n",
      "Input Sentence:newark to cleveland daily <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "True Slots:O B-fromloc.city_name O B-toloc.city_name B-flight_days <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Pred Slots:O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "True Intent:atis_flight\n",
      "\n",
      "Pred Intent:atis_flight\n",
      "slot accuracy: 0.8579545454545454, intent accuracy: 1.0\n",
      "slot accuracy: 0.9106145251396648, intent accuracy: 1.0\n",
      "slot accuracy: 0.8967391304347826, intent accuracy: 0.9375\n",
      "slot accuracy: 0.8770053475935828, intent accuracy: 0.9375\n",
      "slot accuracy: 0.8465346534653465, intent accuracy: 0.875\n",
      "slot accuracy: 0.9, intent accuracy: 0.9375\n",
      "slot accuracy: 0.8700564971751412, intent accuracy: 0.9375\n",
      "slot accuracy: 0.8229665071770335, intent accuracy: 0.9375\n",
      "slot accuracy: 0.8730964467005076, intent accuracy: 0.9375\n",
      "slot accuracy: 0.8830409356725146, intent accuracy: 0.8125\n",
      "slot accuracy: 0.8325123152709359, intent accuracy: 0.9375\n",
      "slot accuracy: 0.8869047619047619, intent accuracy: 0.8125\n",
      "slot accuracy: 0.9, intent accuracy: 0.875\n",
      "slot accuracy: 0.8407079646017699, intent accuracy: 0.875\n",
      "slot accuracy: 0.825, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9036144578313253, intent accuracy: 1.0\n",
      "slot accuracy: 0.8529411764705882, intent accuracy: 0.8125\n",
      "slot accuracy: 0.8352272727272727, intent accuracy: 0.875\n",
      "slot accuracy: 0.8888888888888888, intent accuracy: 0.9375\n",
      "slot accuracy: 0.8820224719101124, intent accuracy: 1.0\n",
      "slot accuracy: 0.8802083333333334, intent accuracy: 0.9375\n",
      "slot accuracy: 0.88268156424581, intent accuracy: 0.8125\n",
      "slot accuracy: 0.8908045977011494, intent accuracy: 0.875\n",
      "slot accuracy: 0.907608695652174, intent accuracy: 0.8125\n",
      "slot accuracy: 0.8630952380952381, intent accuracy: 0.875\n",
      "slot accuracy: 0.8707865168539326, intent accuracy: 0.9375\n",
      "slot accuracy: 0.8894736842105263, intent accuracy: 0.8125\n",
      "slot accuracy: 0.8769230769230769, intent accuracy: 0.75\n",
      "slot accuracy: 0.8983050847457628, intent accuracy: 0.9375\n",
      "slot accuracy: 0.8703703703703703, intent accuracy: 1.0\n",
      "slot accuracy: 0.88125, intent accuracy: 0.6875\n",
      "F1 score for epoch 0: 0.8537021969080555\n",
      "Epoch:1, train_loss:0.6241596981612557\n",
      "Input Sentence:where do the flights from boston to oakland stop <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "True Slots:O O O O O O B-fromloc.city_name O B-toloc.city_name B-flight_stop <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Pred Slots:O O O O O O B-fromloc.city_name O B-toloc.city_name O <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "True Intent:atis_airport\n",
      "\n",
      "Pred Intent:atis_flight\n",
      "slot accuracy: 0.9385474860335196, intent accuracy: 0.8125\n",
      "slot accuracy: 0.9453551912568307, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9385474860335196, intent accuracy: 0.75\n",
      "slot accuracy: 0.9440993788819876, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9203980099502488, intent accuracy: 0.875\n",
      "slot accuracy: 0.9310344827586207, intent accuracy: 0.9375\n",
      "slot accuracy: 0.8970588235294118, intent accuracy: 0.875\n",
      "slot accuracy: 0.9337016574585635, intent accuracy: 1.0\n",
      "slot accuracy: 0.8586387434554974, intent accuracy: 0.875\n",
      "slot accuracy: 0.9473684210526315, intent accuracy: 1.0\n",
      "slot accuracy: 0.9459459459459459, intent accuracy: 0.9375\n",
      "slot accuracy: 0.91701244813278, intent accuracy: 1.0\n",
      "slot accuracy: 0.9166666666666666, intent accuracy: 1.0\n",
      "slot accuracy: 0.96045197740113, intent accuracy: 1.0\n",
      "slot accuracy: 0.9302325581395349, intent accuracy: 0.9375\n",
      "slot accuracy: 0.8870056497175142, intent accuracy: 0.8125\n",
      "slot accuracy: 0.8758620689655172, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9380530973451328, intent accuracy: 1.0\n",
      "slot accuracy: 0.9622641509433962, intent accuracy: 1.0\n",
      "slot accuracy: 0.9572649572649573, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9447513812154696, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9878048780487805, intent accuracy: 1.0\n",
      "slot accuracy: 0.9375, intent accuracy: 1.0\n",
      "slot accuracy: 0.9265536723163842, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9431818181818182, intent accuracy: 1.0\n",
      "slot accuracy: 0.9333333333333333, intent accuracy: 1.0\n",
      "slot accuracy: 0.9210526315789473, intent accuracy: 1.0\n",
      "slot accuracy: 0.9192546583850931, intent accuracy: 0.875\n",
      "slot accuracy: 0.9301075268817204, intent accuracy: 1.0\n",
      "slot accuracy: 0.9162011173184358, intent accuracy: 1.0\n",
      "slot accuracy: 0.9454545454545454, intent accuracy: 0.9375\n",
      "F1 score for epoch 1: 0.9173983739837398\n",
      "Epoch:2, train_loss:0.30605267483904935\n",
      "Input Sentence:what airlines fly from boston to denver <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "True Slots:O O O O O B-fromloc.city_name O B-toloc.city_name <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Pred Slots:O O O O O B-fromloc.city_name O B-toloc.city_name <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "True Intent:atis_airline\n",
      "\n",
      "Pred Intent:atis_airline\n",
      "slot accuracy: 0.9554140127388535, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9782608695652174, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9615384615384616, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9763313609467456, intent accuracy: 1.0\n",
      "slot accuracy: 0.9318181818181818, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9772727272727273, intent accuracy: 1.0\n",
      "slot accuracy: 0.9759036144578314, intent accuracy: 0.9375\n",
      "slot accuracy: 0.96045197740113, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9693251533742331, intent accuracy: 0.8125\n",
      "slot accuracy: 0.9823529411764705, intent accuracy: 1.0\n",
      "slot accuracy: 0.9653465346534653, intent accuracy: 1.0\n",
      "slot accuracy: 0.8986175115207373, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9591836734693877, intent accuracy: 1.0\n",
      "slot accuracy: 0.9261083743842364, intent accuracy: 1.0\n",
      "slot accuracy: 0.9585492227979274, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9482758620689655, intent accuracy: 1.0\n",
      "slot accuracy: 0.9707602339181286, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9682539682539683, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9707317073170731, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9424083769633508, intent accuracy: 1.0\n",
      "slot accuracy: 0.9743589743589743, intent accuracy: 1.0\n",
      "slot accuracy: 0.953757225433526, intent accuracy: 1.0\n",
      "slot accuracy: 0.9313725490196079, intent accuracy: 0.875\n",
      "slot accuracy: 0.9389671361502347, intent accuracy: 1.0\n",
      "slot accuracy: 0.9347826086956522, intent accuracy: 1.0\n",
      "slot accuracy: 0.9562841530054644, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9523809523809523, intent accuracy: 1.0\n",
      "slot accuracy: 0.949438202247191, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9567567567567568, intent accuracy: 0.875\n",
      "slot accuracy: 0.9505494505494505, intent accuracy: 1.0\n",
      "slot accuracy: 0.9662921348314607, intent accuracy: 0.875\n",
      "F1 score for epoch 2: 0.9455461638491548\n",
      "Epoch:3, train_loss:0.16875435097257135\n",
      "Input Sentence:what fare codes <UNK> flights from philadelphia to san francisco <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "True Slots:O O O O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Pred Slots:O O O O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "True Intent:atis_abbreviation\n",
      "\n",
      "Pred Intent:atis_airfare\n",
      "slot accuracy: 0.965, intent accuracy: 0.875\n",
      "slot accuracy: 0.98125, intent accuracy: 1.0\n",
      "slot accuracy: 0.9825581395348837, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9456521739130435, intent accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slot accuracy: 0.9759036144578314, intent accuracy: 1.0\n",
      "slot accuracy: 0.9375, intent accuracy: 1.0\n",
      "slot accuracy: 0.9696969696969697, intent accuracy: 1.0\n",
      "slot accuracy: 0.9693251533742331, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9761904761904762, intent accuracy: 1.0\n",
      "slot accuracy: 0.9945054945054945, intent accuracy: 1.0\n",
      "slot accuracy: 0.9595959595959596, intent accuracy: 1.0\n",
      "slot accuracy: 0.9061032863849765, intent accuracy: 1.0\n",
      "slot accuracy: 0.96875, intent accuracy: 1.0\n",
      "slot accuracy: 0.946524064171123, intent accuracy: 1.0\n",
      "slot accuracy: 0.9259259259259259, intent accuracy: 0.8125\n",
      "slot accuracy: 0.9493670886075949, intent accuracy: 1.0\n",
      "slot accuracy: 0.9712643678160919, intent accuracy: 1.0\n",
      "slot accuracy: 0.9619565217391305, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9774011299435028, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9569377990430622, intent accuracy: 1.0\n",
      "slot accuracy: 0.9934640522875817, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9252873563218391, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9842931937172775, intent accuracy: 0.8125\n",
      "slot accuracy: 0.9669811320754716, intent accuracy: 1.0\n",
      "slot accuracy: 0.9814814814814815, intent accuracy: 1.0\n",
      "slot accuracy: 0.9346733668341709, intent accuracy: 1.0\n",
      "slot accuracy: 0.9439655172413793, intent accuracy: 1.0\n",
      "slot accuracy: 0.9562841530054644, intent accuracy: 1.0\n",
      "slot accuracy: 0.9689119170984456, intent accuracy: 1.0\n",
      "slot accuracy: 0.9700598802395209, intent accuracy: 1.0\n",
      "slot accuracy: 0.9590643274853801, intent accuracy: 1.0\n",
      "F1 score for epoch 3: 0.9550735234381346\n",
      "Epoch:4, train_loss:0.10405633581749248\n",
      "Input Sentence:i'd like to find a flight from las vegas to detroit michigan that leaves in the afternoon on monday <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "True Slots:O O O O O O O O B-fromloc.city_name I-fromloc.city_name O B-toloc.city_name B-toloc.state_name O O O O B-depart_time.period_of_day O B-depart_date.day_name <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Pred Slots:O O O O O O O O B-fromloc.city_name I-fromloc.city_name O B-toloc.city_name B-toloc.state_name O O O O B-arrive_time.period_of_day O B-arrive_date.day_name <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "True Intent:atis_flight\n",
      "\n",
      "Pred Intent:atis_flight\n",
      "slot accuracy: 0.9463414634146341, intent accuracy: 1.0\n",
      "slot accuracy: 0.9765258215962441, intent accuracy: 1.0\n",
      "slot accuracy: 0.9787234042553191, intent accuracy: 1.0\n",
      "slot accuracy: 0.9526315789473684, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9352941176470588, intent accuracy: 1.0\n",
      "slot accuracy: 0.9884393063583815, intent accuracy: 1.0\n",
      "slot accuracy: 0.985, intent accuracy: 1.0\n",
      "slot accuracy: 0.9658536585365853, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9565217391304348, intent accuracy: 0.8125\n",
      "slot accuracy: 0.9707602339181286, intent accuracy: 1.0\n",
      "slot accuracy: 0.9894179894179894, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9636363636363636, intent accuracy: 1.0\n",
      "slot accuracy: 0.9289340101522843, intent accuracy: 0.9375\n",
      "slot accuracy: 0.926829268292683, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9811320754716981, intent accuracy: 1.0\n",
      "slot accuracy: 0.9884393063583815, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9888268156424581, intent accuracy: 1.0\n",
      "slot accuracy: 0.9946236559139785, intent accuracy: 1.0\n",
      "slot accuracy: 0.9536423841059603, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9690721649484536, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9931972789115646, intent accuracy: 1.0\n",
      "slot accuracy: 0.9636363636363636, intent accuracy: 1.0\n",
      "slot accuracy: 0.9866666666666667, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9779005524861878, intent accuracy: 1.0\n",
      "slot accuracy: 0.9940476190476191, intent accuracy: 0.9375\n",
      "slot accuracy: 0.98, intent accuracy: 1.0\n",
      "slot accuracy: 0.9871794871794872, intent accuracy: 1.0\n",
      "slot accuracy: 0.9946808510638298, intent accuracy: 1.0\n",
      "slot accuracy: 0.9878787878787879, intent accuracy: 1.0\n",
      "slot accuracy: 0.9537037037037037, intent accuracy: 0.9375\n",
      "slot accuracy: 0.9659090909090909, intent accuracy: 1.0\n",
      "F1 score for epoch 4: 0.9648951730862994\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        train_loss, mean_loss = 0, 0\n",
    "        \n",
    "        for epoch in range(epoch_num):\n",
    "            \n",
    "            # training\n",
    "            for i,training_data in enumerate(get_batch(train_data_with_id, batch_size)):\n",
    "                training_data = list(zip(*training_data))\n",
    "                run_target = [train_op, loss]\n",
    "                fd = {\n",
    "                    encoder_input:np.transpose(training_data[0], [1, 0]),\n",
    "                    encoder_len:training_data[1],\n",
    "                    slot_target:training_data[2],\n",
    "                    intent_target:training_data[3]\n",
    "                }\n",
    "\n",
    "                _, loss_val = sess.run(run_target, feed_dict = fd)\n",
    "                train_loss+=loss_val\n",
    "                mean_loss+=loss_val\n",
    "                \n",
    "                if i%30 == 0:\n",
    "                    mean_loss/=30.0\n",
    "            train_loss/=(i+1)\n",
    "            print(\"Epoch:{}, train_loss:{}\".format(epoch, train_loss))\n",
    "            \n",
    "            # testing\n",
    "            all_pred_slots = []\n",
    "            for i,testing_data in enumerate(get_batch(test_data_with_id, batch_size)):\n",
    "                testing_data = list(zip(*testing_data))\n",
    "                run_target = [slot_prediction_id, intent_prediction_id]\n",
    "                fd = {\n",
    "                    encoder_input:np.transpose(testing_data[0], [1, 0]),\n",
    "                    encoder_len:testing_data[1],\n",
    "                }\n",
    "                \n",
    "                slot_prediction_batch, intent_prediction_batch = sess.run(run_target, feed_dict = fd)\n",
    "                slot_prediction_batch = np.transpose(slot_prediction_batch, [1, 0])\n",
    "                \n",
    "                if i == 0:\n",
    "                    random_index = random.choice(range(len(testing_data)))\n",
    "                    print(\"Input Sentence:{}\".format(seq_index2word(testing_data[0][random_index], id2word)))\n",
    "                    print()\n",
    "                    print(\"True Slots:{}\".format(seq_index2slot(testing_data[2][random_index], id2slot)))\n",
    "                    print()\n",
    "                    print(\"Pred Slots:{}\".format(seq_index2slot(slot_prediction_batch[random_index], id2slot)))\n",
    "                    print()\n",
    "                    print(\"True Intent:{}\".format(id2intent.get(testing_data[3][random_index])))\n",
    "                    print()\n",
    "                    print(\"Pred Intent:{}\".format(id2intent.get(intent_prediction_batch[random_index])))\n",
    "                \n",
    "                slot_true = np.array((testing_data[2]))\n",
    "                \n",
    "                slot_prediction_len = list(np.shape(slot_prediction_batch))[1]\n",
    "                slot_true_len = np.array((testing_data[1]))\n",
    "                \n",
    "                # remove padding\n",
    "                slot_true_with_true_len = slot_true[:, :slot_prediction_len]\n",
    "                \n",
    "                # add padding\n",
    "                slot_prediction_padd = np.lib.pad(slot_prediction_batch, ((0,0), (0, input_steps - slot_prediction_len)), mode = 'constant', constant_values = 0)\n",
    "                all_pred_slots.append(slot_prediction_padd)\n",
    "                \n",
    "                # without padding\n",
    "                slot_accuracy = accuracy_score(slot_true_with_true_len, slot_prediction_batch, slot_true_len)\n",
    "                intent_accuracy = accuracy_score(testing_data[3], intent_prediction_batch)\n",
    "                print(\"slot accuracy: {}, intent accuracy: {}\".format(slot_accuracy, intent_accuracy))\n",
    "                \n",
    "            all_pred_slots = np.vstack(all_pred_slots)\n",
    "            all_true_slots = np.array(list(zip(*test_data_with_id))[2])[:all_pred_slots.shape[0]]\n",
    "            print(\"F1 score for epoch {}: {}\".format(epoch, f1_score_for_seq_batch(all_true_slots, all_pred_slots)))\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
